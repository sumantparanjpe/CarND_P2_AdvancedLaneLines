{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "\n",
    "## Project: **Advanced Lane Lines on the Road** \n",
    "***\n",
    "This is a CV based lane-marker detection and tracking implementation using standard OpenCV packages and advanced image processing functions. \n",
    "\n",
    "The entire pipeline is structured as follows. \n",
    "\n",
    "---\n",
    ">**Advanced Lane Finding Project**\n",
    "\n",
    ">The goals / steps of this project are the following:\n",
    "\n",
    ">1. Camera Calibration\n",
    ">    1. Compute the camera calibration matrix and distortion coefficients given a set of chessboard images\n",
    ">    2. Apply a distortion correction to raw images\n",
    ">2. Color Space & Gradient Processing\n",
    ">    1. Use color transforms in HLS to extract useful color/level data   \n",
    ">    2. Use gradients with Sobel operator and thresholding for a binary image\n",
    ">    3. Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    ">4. Detect lane pixels and fit to find the lane boundary.\n",
    ">     1. Determine the curvature of the lane and vehicle position with respect to center.\n",
    ">     2. Warp the detected lane boundaries back onto the original image.\n",
    ">     3. Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.  \n",
    ">5. Video Processing Pipeline   \n",
    ">     1. Image undistort\n",
    ">     2. Color transform (use S-channel)\n",
    ">     3. Gradient detector for edges\n",
    ">     4. Perspective transform to dewarp camera image\n",
    ">     5. Lane detect using historical tracking data\n",
    ">     6. Calcualte curvature metrics\n",
    ">     7. Warp back to original image geometry\n",
    "***\n",
    "\n",
    "\n",
    "## 1. Camera Calibration\n",
    "\n",
    "Use checkerboard images to calibrate camera intrinsics. \n",
    "* Load reference calibration images\n",
    "* Detect checkerboard corners for _valid_ images & calibrate camera sensor\n",
    "* Undistort images using calibration matrix (instrinsics)\n",
    "\n",
    "- - - - \n",
    "\n",
    "### 1.1 Camera Calibration & Distortion Correction\n",
    "A sample calibration data frame used for camera matrix calculation. \n",
    "\n",
    "<img src=\"./Resources/camera_cal/calibration3.jpg\" alt=\"Calibration Data Frame\" width=450/>\n",
    "\n",
    "Checkerboard detection usign OpenCV library, the camera fundamental matrix and distortion transformations are derived. We test these on the sample images to get the undistorted frame. A sample is provided below. \n",
    "\n",
    "<img src=\"./Resources/test_images_undistort/straight_lines1.jpg\" alt=\"Undistorted Test Image\" width=450/>\n",
    "\n",
    "## 2. Color & Gradient Processing\n",
    "The main task for this process pipeline is to extract the relevant lane-markers from the test images, the dominant colors for this selection being white & yelow.\n",
    "\n",
    "After a suitable regio-of-interest mask and edge gradient thresholding, a bird's eye view is obtained by a warping transformation. The reason for this is to make it easier to detect and track lane marker lines across a camera frame by transforming the geometrically warped image into a rectangular view.    \n",
    "\n",
    "### 2.1 Color Thresholding & Gradient Detection\n",
    "The undistorted frames are then processed to pick up the white & yellow colors dominant in lane markers. Various color spaces were evaluated with a threshold selector for `RGB`, `HSV` and `Lab` color components.\n",
    "\n",
    "A alternative color thresholding using `R` and `S` channels was also tried but the more robust 'white-yellow' color selector using the following thresholds was found ot be most robust. \n",
    "\n",
    "Using white & yellow thresholding on test images with the following component thresholds results in the following binary image for the sample test frame. \n",
    "\n",
    "| Color | Space | Min Threshold | Max Threshold |\n",
    "|-:|-:|-:|-:|\n",
    "|White|RGB|[100, 100, 200]|[255, 255, 255]|\n",
    "|Yellow|RGB|[225, 180, 0]|[255, 255, 170]|\n",
    "|Yellow|HLS|[20, 120, 130]|[45, 200, 255]|\n",
    "|Yellow|Lab|[0, 0, 154]|[0, 0, 255]|\n",
    "\n",
    "<img src=\"./Resources/test_images_undistort_whiteyellow/straight_lines1.jpg\" alt=\"Binary Thresholded Test Image\" width=450/>\n",
    "\n",
    "### 2.2 RoI Mask & Gradient Thresholding\n",
    "The frames are then processed on the selected RoI that maximizes the probability of lane marker detection while reducing unnecessary processing on parts of the image that are not likely to contain any useful information. \n",
    "\n",
    "The gradient detectors used were evaluated from `sobel'` and `laplacian` operators with various kernel sizes. A separate absolute, magnitude and angular threshold selection was done across the thresholded image. \n",
    "\n",
    "The RoI selected and gradient thresholded image used for further processing is sampled below. \n",
    "\n",
    "<img src=\"./Resources/test_images_gradient_rhs_grad/straight_lines1.jpg\" alt=\"Gradient Threhsolded Test Image\" width=450/>\n",
    "\n",
    "### 2.3 De-warping with Perspective Transform\n",
    "To aid in the lane marker detection, the gradient thresholded image is then de-warped using the camera distortion matrix computed in the earlier steps and `pickled`. \n",
    "\n",
    "For this process, we select a test image that shows the largest span of straight lane lines as below. \n",
    "\n",
    "<img src=\"./Resources/test_images_warped_gradient_binary/straight_lines1_linemarked.jpg\" alt=\"Straight Line Marked Test Image\" width=450/>\n",
    "\n",
    "These are then mapped to a rectangular pixel region to generate the de-warping transformation. Both the warp and inverse-warp (de-warp) transformations are `pickled` for further processing. This process is done just once per camera setup and hence not part of the pipeline process used for the video frames. \n",
    "\n",
    "<img src=\"./Resources/test_images_warped_gradient_binary/straight_lines1_warped.jpg\" alt=\"Warped (Bird's-eye View) Test Image\" width=450/>\n",
    "\n",
    "The binary gradient thresholded frame is then warped using the same warp transformation computed above to provide a bird's-eye view of lane marker lines. \n",
    "\n",
    "<img src=\"./Resources/test_images_warped_gradient_binary/straight_lines1.jpg\" alt=\"Warped (Bird's-eye View) Gradient Test Image\" width=450/>\n",
    "\n",
    "## 3. Lane Boundary Detector\n",
    "A couple of lane detector and tracking algorithms were tested based on the theory & practice covered in the course material. \n",
    "\n",
    "The primary lane marker detector before applying the searcher-tracker is asimple historgram peak search that picks out the lanes by detecting the two dominant peaks in the gradient thresholded image as shown below.\n",
    "\n",
    "<img src=\"./Resources/test_images_warped_gradient_binary/straight_lines1_warped_histogram.jpg\" alt=\"Warped Lane Histogram (bottom 1/4th)\" width=450/>\n",
    "\n",
    "\n",
    "### 3.1 Sliding Window Tracker\n",
    "The sliding window tracker first sets up a series of search window pairs, one each for left and the right lanes that split up the entire frame into a given number of horizontal slices. This limits the search region for each lane to a small rectangular area that can work robustly for curved lanes. \n",
    "\n",
    "All non-zero binary gradient thresholded pixels that lie within the search rectangle are marked as `lane` pixels. As the slices are scanned from bottom to top, the next slice window centers are adjusted to be the mean of the previous slice lane pixels, but only if the count exceed some minimum detection threshold. \n",
    "\n",
    "The number of slices, the thresholds and the search window dimensions are all tunable and have been chosen after some exploration of across the test image & video data set. \n",
    "\n",
    "A second order polynomial is fit through all detected left & right lane pixels to provide an anlytic expression of the lane markers. \n",
    "\n",
    "A sample window search with the rectangular search windows and the identified lane pixels and markers is shown below.\n",
    "\n",
    "<img src=\"./Resources/test_images_slidingwindow_lanemarkers/straight_lines1.jpg\" alt=\"Sliding Window Lane-marker Search\" width=450/>\n",
    "\n",
    "Using a search window around the previous frame's detected polynomial allows a much tighter and temporally correlated detection. The plot below shows in green the region of search which takes advantage of the temporal correlation between video frames and hence can be a whole lot tighter than a brute force blank search.\n",
    "\n",
    "* Histogram peak line detect\n",
    "* Line tracking over frame using sliding window & convolution search algorithm\n",
    "* Measure curvature and car body center markers\n",
    "\n",
    ">**Note**: Experiments with various lane weighting combinations were tried and the following method was used to `weigh` the relative importance of detected lane pixels while estimating the 2nd order polynomials used to define the left & right lane markers. \n",
    "> 1. Detected lane pixels *closer* to the camera (i.e. lower in the image frame) are weighted exponentially higher than those farther away. `scl_fct = np.log(nwindows-window)`\n",
    "> 2. A higher confidence metric for a lane determines its weight when using the polynomial 2nd and 1st order coefficients. The *weaker* or less confident lane marker uses the average of its own and the dominant lane marker coefficients. \n",
    "\n",
    "<img src=\"./Resources/test_images_slidingwindow_lanemarkers/straight_lines1_history.jpg\" alt=\"Sliding Window Lane-marker Search (with history)\" width=450/>\n",
    "\n",
    "\n",
    "### 3.2 Convolutional Search Tracker\n",
    "The convolutional search algorithm on the other hand uses a mask to convolve the thresholded binary image with a window mask that shows peak where the mask and lane marker pixels coincide. \n",
    "\n",
    "Various order polynomials were tested with weighted centroids (i.e. selecting centroids where normalized convolution peak was a least 30%-tile of the centroids in all the selected search windows for each of left & right lanes). A final selection was made with 2nd order polynomials as below.   \n",
    "\n",
    "<img src=\"./Resources/test_images_slidingwindow_lanemarkers/straight_lines1_conv.jpg\" alt=\"Convolutional Lane-marker Search\" width=450/>\n",
    "\n",
    "### 3.3 Lane Marker Annotation\n",
    "Once the lane markers are detected and modelled, the drivable surface can then be idnetified as the surface encompassing the left & right polynomials. The lanes & the surface can then be warped back to the original space using the `pickled` geometry inversion matrix. \n",
    "\n",
    "With the left & right lane polynomials evaluated, the radius of curvature is then computed in pixel & `km` domain. Other useful metrics such as deviation from center track can also be then annotated onto the frame. \n",
    "\n",
    "<img src=\"./Resources/test_images_annotated_lanes/straight_lines1.jpg\" alt=\"Annotated Lane-marker Detection\" width=450/>\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Video Pipeline Implementation\n",
    "The video pipeline is then just a sequential function call of the various tuned implementations discussed above. These have been implemened in the jupyter notebook [here](.\\VideoPipeline.ipynb)\n",
    "\n",
    "The project test vidoe output processed from the pipeline is availabele [here](./Resources/test_videos_output/project_video.mp4)\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Discussion\n",
    "\n",
    "A few enhancements that can make the implementation more robust:\n",
    "\n",
    "    1. A combined sliding window and convolution search dynamically selected when one fails to detect reliable lane markers. \n",
    "    2. Using a frame averaged lines from multiple previous frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
